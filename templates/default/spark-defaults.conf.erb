# https:/.hops.spark.apache.org/docs/1.2.0/running-on-yarn.html

spark.master                              <%= @master_ip %>
spark.driver.memory                       <%= node['hadoop_spark']['driver_memory'] %>
spark.executor.memory                     <%= node['hadoop_spark']['executor_memory'] %>
spark.eventLog.enabled                    <%= node['hadoop_spark']['eventlog_enabled'] %>
spark.eventLog.dir                        hdfs://<%= @nn_endpoint %><%= @eventlog_dir %>
spark.serializer                          org.apache.spark.serializer.KryoSerializer
spark.worker.cleanup.enabled              <%= node['hadoop_spark']['worker']['cleanup']['enabled'] %>
spark.io.compression.codec                <%= node['hadoop_spark']['io']['compression']['codec'] %>
spark.streaming.stopGracefullyOnShutdown  <%= node['hadoop_spark']['streaming']['stopGracefullyOnShutdown'] %>

# spark-history-server settings
spark.history.fs.logDirectory     hdfs://<%= @nn_endpoint %><%= @eventlog_dir %>
spark.history.fs.cleaner.enabled  <%= node['hadoop_spark']['history']['fs']['cleaner']['enabled'] %>
spark.history.fs.cleaner.interval <%= node['hadoop_spark']['history']['fs']['cleaner']['interval'] %>
spark.history.fs.cleaner.maxAge   <%= node['hadoop_spark']['history']['fs']['cleaner']['maxAge'] %>
spark.eventLog.compress           true
spark.yarn.historyServer.address  <%= @historyserver_endpoint %>

spark.metrics.conf                <%= node['hadoop_spark']['conf_dir'] %>/metrics.properties

# YARN settings

<% auth = if node['hadoop_spark']['yarn']['support'].eql? "true"
                   "spark.authenticate = true"
               else
                   "spark.authenticate.secret = #{node['hadoop_spark']['authenticate']['secret']}"
               end
-%>
<%= auth  %>

spark.yarn.am.waitTime                     <%= node['hadoop_spark']['yarn']['am']['waitTime'] %>
spark.yarn.submit.file.replication         <%= node['hadoop_spark']['yarn']['submit']['file']['replication'] %>
spark.yarn.preserve.staging.files          <%= node['hadoop_spark']['yarn']['preserve']['staging']['files'] %>
spark.yarn.scheduler.heartbeat.interval-ms <%= node['hadoop_spark']['yarn']['scheduler']['heartbeat']['interval_ms'] %>
spark.yarn.queue                           <%= node['hadoop_spark']['yarn']['queue'] %>
spark.yarn.containerLauncherMaxThreads     <%= node['hadoop_spark']['yarn']['containerLauncherMaxThreads'] %>

spark.sql.catalogImplementation            hive
spark.sql.warehouse.dir                    hdfs://<%= @nn_endpoint %><%= node['hadoop_spark']['yarn']['warehouse_hdfs'] %>
spark.yarn.jars                            <%= node['hadoop_spark']['yarn']['jars'] %>

<% for env in node['hadoop_spark']['yarn']['appMasterEnv'].keys -%>
spark.yarn.appMasterEnv.<% env -%>         <% node['hadoop_spark']['yarn']['appMasterEnv'][env] -%>
<% end -%>

spark.driver.maxResultSize               <%= node['hadoop_spark']['driver']['maxResultSize'] %>
spark.sql.broadcastTimeout               <%= node['hadoop_spark']['sql']['broadcastTimeout'] %>
